{"cells":[{"cell_type":"markdown","metadata":{"id":"bu1wrn9-xcyi"},"source":["#**Deep Q Learning(CartPole)**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4JCCrMfliHNK"},"source":["# 1. Import dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"E7C9_lB1iHNN","executionInfo":{"status":"ok","timestamp":1718694403564,"user_tz":-210,"elapsed":4578,"user":{"displayName":"Ameneh Rezayof","userId":"09143694459640068176"}}},"outputs":[],"source":["import random\n","import gym\n","import numpy as np\n","from collections import deque\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Nadam\n","from keras.optimizers import Adam\n","from IPython.display import HTML\n","import os\n"]},{"cell_type":"markdown","metadata":{"id":"f1_GzakuiHNV"},"source":["# 2. Hyperparameters and initialise environment"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_2sPz36XiHNW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"458ee4a6-ba2f-4ef5-96a0-48849ead914b","executionInfo":{"status":"ok","timestamp":1718694404357,"user_tz":-210,"elapsed":11,"user":{"displayName":"Ameneh Rezayof","userId":"09143694459640068176"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"source":["env = gym.make('CartPole-v0')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"cK0V5UQXiHNd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8f594ec1-c718-431d-9277-3517168c9910","executionInfo":{"status":"ok","timestamp":1718694404358,"user_tz":-210,"elapsed":8,"user":{"displayName":"Ameneh Rezayof","userId":"09143694459640068176"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":3}],"source":["state_size = env.observation_space.shape[0]\n","state_size"]},{"cell_type":"markdown","metadata":{"id":"LVEH7oQExcy7"},"source":["# 3. The agent can take two actions-right or left"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"T3ad_bEGiHNm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a1de133a-ad7f-4070-d67f-9d5ac7d3a14f","executionInfo":{"status":"ok","timestamp":1718694405072,"user_tz":-210,"elapsed":5,"user":{"displayName":"Ameneh Rezayof","userId":"09143694459640068176"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":4}],"source":["action_size = env.action_space.n\n","action_size\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"i0JRMP1diHNr","executionInfo":{"status":"ok","timestamp":1718694406559,"user_tz":-210,"elapsed":4,"user":{"displayName":"Ameneh Rezayof","userId":"09143694459640068176"}}},"outputs":[],"source":["batch_size = 32"]},{"cell_type":"markdown","metadata":{"id":"HOoxZN9Bxcy8"},"source":["# 4. Number of games that the agent is going to play"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"8DDZrj7iiHNw","executionInfo":{"status":"ok","timestamp":1718694408139,"user_tz":-210,"elapsed":10,"user":{"displayName":"Ameneh Rezayof","userId":"09143694459640068176"}}},"outputs":[],"source":["n_episodes = 200"]},{"cell_type":"markdown","source":["##5. Define the DQNAgent Clas"],"metadata":{"id":"HKppydZADHTx"}},{"cell_type":"markdown","metadata":{"id":"9EJxD9Ksxcy-"},"source":["6.DQNAgent\n","Step 1:  Python class is used to define our DQNAgent.\n","Step 2 : Neural Net to approximate Q-value function. Q is a function of s,a,theta.\n","Step 3: List of previous experiences, enabling re-training later.\n","Step 4:  Train NN with experiences sampled from memory.\n","Step 5: We save our model so we can use it later.\n","\n","[1].We are using TensorFlow and Keras modules to build the neural network components of our Deep Q-Learning Network. The first thing to do is to set our hyperparameters. So, we are using the \"OpenAI gym make method.\" So, we import OpenAI gym here, and we are using the make method that it provides to select the environment of our choice. We will be using the Cartpole game. We then get our state information. We inquire the environment as to what the possible state space size is and remember that in the Cartpole game, there are only four. It is the cart position, the cart velocity, the pole angle, and the pole angular velocities. Thus, these are the four pieces of information we have here. We also have two possible actions that we can take. We can move left, or we can move right. When we train our model using stochastic gradient descent, the batch size hyperparameter is yours to play around with. Thirty-two worked fine. You could go up to 128, probably without much issue. The number of episodes is how many rounds of gameplay we want our agent to play. We will train for 1000 rounds of the game. So, we will allow it to die or win the game a thousand times. We have an output directory for storing model outputs. So, we will save model weights in a unique directory called Cartpole. After every few 100 episodes of training. If this directory does not yet exist, then please make it exist. Now we are using a python class to define or Deep Q-Learning Network agent. We are going to go through them one by one here. So, when we initialize our agent, we pass in the state size as well as the action size. And then, we set those as attributes for the class. We have memory, which we will use to do something called \"memory replay\" later. So, what we are going to do is go to the store up to 2000 memories at any given time point. A deque is just like a list, except that we can set a maximum size to the list. We append memories to our list. First, we have one, then two, then three. Once we get up to 1998-1999, we add our 2000th memory. That is the maximum amount that we can put on this list. So, as we add more items to the list after we already have 2000, we retain the latest memories, but we ditch the oldest memories.\n","\n","[2].The gamma term that we talked about with respect to discounted future reward. The gamma term is going to discount our future reward. Another hyperparameter associated with Deep Q-Learning is the epsilon hyperparameter. So this is the exploration rate. Our exploration rate is a factor between zero and one that determines on any given move whether we take an exploited action, where we make our best guess as to what the correct action should be and take that action. Alternatively, we take an exploratory action, and when we take an exploratory action, we just randomly take action. Thus, in the beginning, we want to be mostly taking exploratory random actions. However, the longer we train, the more we should be able to exploit what we learned and take these exploitative actions. So, the Cartpole game also involved randomly moving left or right. We set a floor on how low our exploration rate can go. A common minimum floor is 1%. Once we get there, we will take random actions on 1% of our actions. The rate at which we decay from our initial 100% exploration rate down to our 1% exploration rate is determined by epsilon decay. So you set this to some near one value, like .99, .95, .995, .999. So this is the hyperparameter that determines how are exploration rate decays from 100% down to 1%.\n","\n","[3]. We are using the Keras sequential API. The first hidden layer, in my version, consists of 32 ReLU neurons, and we also must specify our input shape. Our input shape is the four states. So we will be using this neural net to map states to actions. Here, we are using a relatively simple neural network. It has two hidden layers, and the first hidden layer has 32 ReLU neurons. The second hidden layer has 32 ReLU neurons. And then, I have an output layer, which has a linear activation, and two neurons. So, you could consider this task to be a binary classification task, where we just have two possible actions, left and right, which are encoded by zero and one, respectively. We are using the linear activation function here because, again, with a Cartpole game, it is just a discrete action left to right. In many environments, we would be using some continuous numeric value, so if you are thinking about a self-driving car, you do not just have a binary left turn right turn. You have some numeric value representing how much you turn the steering column right or turn it left. After designing our neural network architecture, we then compile the model. We are using mean squared error loss because that is the kind of loss we should use when using a linear activation output. We are setting the optimizer to nadam or adam. We would still say that this is a Deep Reinforcement Learning algorithm because, in the way that Deep Learning practitioners talk about deep learning architectures, we would typically say that you need to have at least three hidden layers to call a neural network a deep learning architecture. However, in the reinforcement learning world, anytime we use a neural network within our reinforcement learning model, no matter how shallow it is, even with one hidden layer, we can still call that a deep reinforcement learning model.\n","\n","[4]. Recall that we have our deque of up to a 1000 memories stored in a list-like format. So, at any given time point, we have the current state, we have the action that our agent took given that state, and we have the reward that we received from the environment given the agent's action in that state. Furthermore, we also have the next state, a time step T plus one, that was returned to the agent based on the action it took at the state at time step T.So, done simply indicates whether the game has ended or not. So, these five elements give us our set of items that we can record at any given time point. So we append these memories after every single action that we take, after every single round of gameplay. We use the remember method to remember one more experience. The chunkiest method here is the training method, but it is not that complicated. Our train method is a method that we will use to train our Neural Network with experiences that we sampled from memory. So we start by sampling a minibatch from memory. We randomly sample from our memory deque a batch of the size of our mini-batch size that we decided on for stochastic gradient descent. Once we have that minibatch, we loop over all of the elements, all of the memories within that minibatch, until we have those same five elements that we had stored here within any given memory. So, the first thing that we do is, if the game ended, so if this memory was the final time step in a given a round of gameplay, this is a step in which the agent got to the final round of gameplay. So, it managed to stay alive for 200-time steps, or this is because the game ended early due to the pole moving away from vertical. In that case, if the game is over, then our target reward is equal to whatever the reward that we attained. So, we do not need to guess what our discounted future reward might be. We know precisely what are target reward is. However, if the game is not done, we will calculate the future target reward based on the next state. We will take the reward that we know we have, and then we calculate based on the next state returned. So based on the state at time step T plus one, we use our model to predict the future reward, and we discount that future reward by our discount rate gamma. We will be looking at the results of our agent's performance after every round of gameplay. Note that we are going to be taking exploratory actions all the time. However, as we decay our epsilon rate as we train more and more, we will gradually decrease this epsilon value. Finally, a simple utility method is here for saving model weights at regular intervals. And then, we can choose to load those model weights back in later on. Now We have defined our DQN agent. We are now ready to set up this agent for interacting with an environment.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Uw-8bzLoxcy-","executionInfo":{"status":"ok","timestamp":1718694412595,"user_tz":-210,"elapsed":503,"user":{"displayName":"Ameneh Rezayof","userId":"09143694459640068176"}}},"outputs":[],"source":["class DQNAgent:\n","    def __init__(self, state_size, action_size, **kwargs):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=2000)\n","        self.gamma = 0.95\n","        self.epsilon = 1.0\n","        self.epsilon_decay = 0.99\n","        self.epsilon_min = 0.01\n","        self.model = self._build_model(**kwargs)\n","\n","    def _build_model(self, num_layers, layers_units, activation, learning_rate):\n","        model = Sequential()\n","        model.add(Dense(layers_units[0], activation=activation, input_dim=self.state_size))\n","        for i in range(num_layers - 1):\n","            model.add(Dense(layers_units[i+1], activation=activation))\n","        model.add(Dense(self.action_size, activation='linear'))\n","        model.compile(loss='mse', optimizer=Nadam(learning_rate=learning_rate))\n","        return model\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def train(self, batch_size):\n","        minibatch = random.sample(self.memory, batch_size)\n","        for state, action, reward, next_state, done in minibatch:\n","            target = reward\n","            if not done:\n","                target = (reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0]))\n","            target_f = self.model.predict(state, verbose=0)\n","            target_f[0][action] = target\n","            self.model.fit(state, target_f, epochs=1, verbose=0)\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","    def act(self, state):\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        act_values = self.model.predict(state, verbose=0)\n","        return np.argmax(act_values[0])\n","\n","    def save(self, name):\n","        self.model.save_weights(name)\n","\n","    def load(self, name):\n","        self.model.load_weights(name)"]},{"cell_type":"markdown","source":["- Defines the `DQNAgent` class with methods for building the model, remembering experiences, training, acting, and saving/loading the model."],"metadata":{"id":"cD8QavB8DThj"}},{"cell_type":"markdown","source":["### Define the Tuner Class"],"metadata":{"id":"lxiy-y3MDdye"}},{"cell_type":"code","source":["!pip install keras-tuner"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QhQsxB9_K6ka","executionInfo":{"status":"ok","timestamp":1718694442165,"user_tz":-210,"elapsed":6425,"user":{"displayName":"Ameneh Rezayof","userId":"09143694459640068176"}},"outputId":"b01aabac-083d-4e55-c935-348cc7c4704d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["Collecting keras-tuner\n","  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n","Collecting kt-legacy (from keras-tuner)\n","  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.6.2)\n","Installing collected packages: kt-legacy, keras-tuner\n","Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"]}]},{"cell_type":"code","source":["import keras_tuner\n","class Tuner(keras_tuner.RandomSearch):\n","    def run_trial(self, trial, *args, **kwargs):\n","        agent = self.build(trial.hyperparameters, *args)\n","        score = self.fit(agent, **kwargs)\n","        return score\n","\n","    def build(self, hp, *args):\n","        num_layers = hp.Int(\"num_layers\", 1, 2)\n","        layers_units = [hp.Int(\"units_1\", min_value=32, max_value=64, step=32)]\n","        for i in range(num_layers - 1):\n","            layers_units.append(hp.Int(f\"units_{i+2}\", min_value=16, max_value=32, step=16))\n","\n","        return DQNAgent(*args,num_layers=num_layers,layers_units=layers_units,activation=hp.Choice(\"activation\", [\"relu\", \"sigmoid\", \"tanh\"]),learning_rate=hp.Float(\"learning_rate\", min_value=0.001, max_value=0.01, sampling=\"log\"))\n","\n","    def fit(self, agent, n_episodes, batch_size):\n","        latest_scores = np.zeros(int(0.1 * n_episodes))\n","\n","        for e in range(n_episodes):\n","            state = env.reset()\n","            state = np.reshape(state, [1, state_size])\n","\n","            done = False\n","            time = 0\n","            while not done:\n","                action = agent.act(state)\n","                next_state, reward, done, _ = env.step(action)\n","                reward = reward if not done else -10\n","                next_state = np.reshape(next_state, [1, state_size])\n","                agent.remember(state, action, reward, next_state, done)\n","                state = next_state\n","                if done:\n","                    print(\"episode: {}/{}, score: {}, e: {:.2}\"\n","                          .format(e, n_episodes-1, time, agent.epsilon))\n","                time += 1\n","\n","            if e >= int(0.9*n_episodes):\n","                latest_scores[e - int(0.9*n_episodes)] = time - 1\n","\n","            if len(agent.memory) > batch_size:\n","                agent.train(batch_size)\n","\n","        return sum(latest_scores) / len(latest_scores)"],"metadata":{"id":"zL8JBQNEDXhK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718694481840,"user_tz":-210,"elapsed":978,"user":{"displayName":"Ameneh Rezayof","userId":"09143694459640068176"}},"outputId":"939ee3fc-be73-40df-9e6b-4e3e13888f16"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["**Explanation:**\n","- Defines the `Tuner` class which extends `keras_tuner.RandomSearch`.\n","- Implements methods to build and fit the `DQNAgent` model using hyperparameters provided by the tuner."],"metadata":{"id":"zgOIxxfiDn69"}},{"cell_type":"markdown","source":["### Run Hyperparameter Tuning"],"metadata":{"id":"CKrpgto6Dr-L"}},{"cell_type":"code","source":["tuner = Tuner(max_trials=5,\n","              overwrite=True,\n","              objective=keras_tuner.Objective('score', direction=\"max\"))\n","\n","tuner.search(state_size, action_size, n_episodes=200, batch_size=32)\n","tuner.search_space_summary()\n","tuner.results_summary()"],"metadata":{"id":"u0FPi9yADwgm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718702349146,"user_tz":-210,"elapsed":7861383,"user":{"displayName":"Ameneh Rezayof","userId":"09143694459640068176"}},"outputId":"29a7562e-f8f2-4eac-c1e4-2645dc231f7e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Trial 5 Complete [00h 30m 15s]\n","score: 100.15\n","\n","Best score So Far: 100.15\n","Total elapsed time: 02h 11m 00s\n","Search space summary\n","Default search space size: 5\n","num_layers (Int)\n","{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 2, 'step': 1, 'sampling': 'linear'}\n","units_1 (Int)\n","{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 64, 'step': 32, 'sampling': 'linear'}\n","activation (Choice)\n","{'default': 'relu', 'conditions': [], 'values': ['relu', 'sigmoid', 'tanh'], 'ordered': False}\n","learning_rate (Float)\n","{'default': 0.001, 'conditions': [], 'min_value': 0.001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n","units_2 (Int)\n","{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 32, 'step': 16, 'sampling': 'linear'}\n","Results summary\n","Results in ./untitled_project\n","Showing 10 best trials\n","Objective(name=\"score\", direction=\"max\")\n","\n","Trial 4 summary\n","Hyperparameters:\n","num_layers: 2\n","units_1: 32\n","activation: relu\n","learning_rate: 0.004144830366654579\n","units_2: 32\n","Score: 100.15\n","\n","Trial 1 summary\n","Hyperparameters:\n","num_layers: 2\n","units_1: 64\n","activation: tanh\n","learning_rate: 0.0028246325785106557\n","units_2: 16\n","Score: 96.9\n","\n","Trial 3 summary\n","Hyperparameters:\n","num_layers: 2\n","units_1: 64\n","activation: relu\n","learning_rate: 0.001486668133601622\n","units_2: 16\n","Score: 54.95\n","\n","Trial 0 summary\n","Hyperparameters:\n","num_layers: 1\n","units_1: 32\n","activation: relu\n","learning_rate: 0.001\n","Score: 53.05\n","\n","Trial 2 summary\n","Hyperparameters:\n","num_layers: 2\n","units_1: 32\n","activation: sigmoid\n","learning_rate: 0.006488638452453321\n","units_2: 32\n","Score: 32.2\n"]}]},{"cell_type":"markdown","source":["**Explanation:**\n","- Initializes the `Tuner` class with the specified number of trials and objective.\n","- Executes the hyperparameter search with the defined state size, action size, number of episodes, and batch size.\n","- Displays the search space summary and results summary.\n","\n","By organizing the code into these cells, you can run each section step-by-step in a Jupyter notebook, with clear explanations of what each part does. This will help in understanding and debugging the code effectively."],"metadata":{"id":"d-s5k_-jEKs1"}}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"metadata":{"interpreter":{"hash":"1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}